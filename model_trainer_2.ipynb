{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daredevil007-aks/Forex_prediction/blob/main/model_trainer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ1cLTYncfJ9",
        "outputId": "58d68f7e-c77e-4de4-8e64-8fad19d5aa5e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       3.4Gi       7.2Gi       1.0Mi       2.1Gi       9.0Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "def print_memory():\n",
        "    pid = os.getpid()\n",
        "    py = psutil.Process(pid)\n",
        "    memoryUse = py.memory_info()[0] / 2.**30  # memory use in GB\n",
        "    print(f'Memory usage: {memoryUse:.2f} GB')\n",
        "\n",
        "print_memory()\n"
      ],
      "metadata": {
        "id": "uxZRXKjNh8m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UDKhfFwOrKla"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('DAT_MS_EURUSD_M1_202407.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wvfehMtKrKlc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Convert `Datetime` to datetime format\n",
        "data['Datetime'] = pd.to_datetime(data['Datetime'], unit='ms')\n",
        "\n",
        "# Sort data by datetime just in case\n",
        "data = data.sort_values(by='Datetime')\n",
        "\n",
        "# Use only the 'Close' prices for this example\n",
        "prices = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_prices = scaler.fit_transform(prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9Wm4eztYrKld"
      },
      "outputs": [],
      "source": [
        "look_back = 24 * 60  # 5 months of minute data (approx. 30 days per month) -> changing to 24 hours for testing hptuning\n",
        "predict_forward = 5  # Predict next 5 minutes\n",
        "\n",
        "# Ensure there are enough data points\n",
        "if len(scaled_prices) < look_back + predict_forward:\n",
        "    raise ValueError(\"Not enough data points for the given look_back period.\")\n",
        "\n",
        "# Create dataset\n",
        "def create_minute_dataset(dataset, look_back, predict_forward=5):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset) - look_back - predict_forward):\n",
        "        # time.sleep(1)\n",
        "        a = dataset[i:(i + look_back), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[(i + look_back):(i + look_back + predict_forward), 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, Y = create_minute_dataset(scaled_prices, look_back, predict_forward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vr4bl3s9rKld"
      },
      "outputs": [],
      "source": [
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras[tensorflow] --quiet\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ssxB58VEw2uQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Model creation function with **kwargs to accept dynamic parameters\n",
        "def create_model(units=50, optimizer='adam', **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units, return_sequences=True, input_shape=(look_back, 1)))\n",
        "    model.add(LSTM(units, return_sequences=False))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(predict_forward))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the KerasRegressor with model creation function\n",
        "model = KerasRegressor(model=create_model, verbose=0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'model__units': [50, 100],         # Use 'model__' prefix for parameters of the model function\n",
        "    'model__optimizer': ['adam', 'rmsprop'],\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [16, 32]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "\n",
        "# Fit the model\n",
        "X_small = X[:1000]\n",
        "Y_small = Y[:1000]\n",
        "grid_result = grid.fit(X_small, Y_small)\n",
        "\n",
        "\n",
        "# Output the best parameters\n",
        "print(f\"Best parameters: {grid_result.best_params_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJMEiOydyGx9",
        "outputId": "f364c7ce-adf2-40b3-b397-f2adf0df052f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'batch_size': 16, 'epochs': 20, 'model__optimizer': 'adam', 'model__units': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'units': [50, 100, 150],\n",
        "    'batch_size': [1, 5, 10],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=10,\n",
        "                                   cv=3,\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_small, Y_small)"
      ],
      "metadata": {
        "id": "zyCKzMVFiLWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters: \", random_search.best_params_)\n",
        "print(\"Best Score: \", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "plZIoaDx3y3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}